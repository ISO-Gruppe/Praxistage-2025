{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Tokenizers\n",
    "\n",
    "### 1.1 What is a tokenizer?\n",
    "\n",
    "A tokenizer is a tool that breaks down text into smaller units, called tokens, which can be words, subwords, or characters. This process helps in preparing the text for further analysis or processing by language models.\n",
    "\n",
    "![](../obsidian/Excalidraw/Tokenizers.svg)\n",
    "\n",
    "### 1.2 Locating Pre-Trained Tokenizers\n",
    "\n",
    "You can find the appropriate tokenizers for each open-source LLM in the [Hugging Face model hub](https://huggingface.co/models).\n",
    "\n",
    "For instance, to download the tokenizer for the [Deepseek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) model, simply use the following code:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Download tokenizer from Hugging Face\n",
    "tokenizer = Tokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
    "\n",
    "# Encode a sample text\n",
    "tokens = tokenizer.encode(\"This is a sample\")\n",
    "print(f\"IDs: {tokens.ids}\")\n",
    "print(f\"Tokens: {tokens.tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Tokenizers and Languages\n",
    "\n",
    "It's important to note that tokenizers are typically developed before the model's training process, using only a subset of the available data. As a result, each model comes with its own unique \"vocabulary.\"\n",
    "\n",
    "This means that tokenizers may perform significantly worse when processing languages or text types that were not included in the training data.\n",
    "\n",
    "Below, we explore several tokenizers:\n",
    "\n",
    "- [`deepseek-ai/DeepSeek-R1`](https://huggingface.co/deepseek-ai/DeepSeek-R1)\n",
    "- [`google-bert/bert-base-uncased`](https://huggingface.co/google-bert/bert-base-uncased)\n",
    "- [`deepseek-ai/DeepSeek-Coder-V2-Instruct`](https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct)\n",
    "- [`intfloat/multilingual-e5-large`](https://huggingface.co/intfloat/multilingual-e5-large)\n",
    "\n",
    "The code below visualizes these tokenizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize_tokens\n",
    "\n",
    "visualize_tokens(\"This is a sample\", tokenizer_name=\"deepseek-ai/DeepSeek-R1\")\n",
    "visualize_tokens(\"This is a sample\", tokenizer_name=\"google-bert/bert-base-uncased\")\n",
    "visualize_tokens(\"This is a sample\", tokenizer_name=\"deepseek-ai/DeepSeek-Coder-V2-Instruct\")\n",
    "visualize_tokens(\"This is a sample\", tokenizer_name=\"intfloat/multilingual-e5-large\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Your Task:\n",
    "\n",
    "Determine which tokenizer was trained on German texts and which was not. Then, perform the same analysis for code as text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tokenizers using a variety of German texts to identify which model has been trained on German data.\n",
    "# Likewise, test the tokenizers on code samples to determine which model effectively handles code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ollama and LLM hosting\n",
    "\n",
    "### 2.1 What is Ollama?\n",
    "\n",
    "Ollama is a platform designed to simplify the hosting and deployment of large language models, making it easier for developers to integrate powerful AI capabilities into their applications.\n",
    "\n",
    "To verify that Ollama is running properly, use the following command:\n",
    "\n",
    "```bash\n",
    "ollama -v\n",
    "```\n",
    "\n",
    "If the command doesn't return a version number, you'll need to start the Ollama server before proceeding.\n",
    "\n",
    "### 2.2 Hosting a Local LLM\n",
    "\n",
    "Below is an example demonstrating how to download and run Huggingface's [SmolLM2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) locally on your PC.\n",
    "\n",
    "<img src=\"https://cdn-uploads.huggingface.co/production/uploads/61c141342aac764ce1654e43/oWWfzW4RbWkVIo7f-5444.png\" alt=\"SmolLM2 Image\" style=\"width:300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "MODEL = \"smollm2:360m\"\n",
    "# Initialize the Ollama client\n",
    "client = Client(host=\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model and list the downloaded models\n",
    "client.pull(MODEL)\n",
    "client.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your can now chat with the model by sending a message to the server. \n",
    "\n",
    "message = {\"role\": \"user\", \"content\": \"What are you?\"}\n",
    "\n",
    "for part in client.chat(model=MODEL, messages=[message], stream=True, keep_alive=30):\n",
    "    print(part[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Langchain and LLM Integration  \n",
    "\n",
    "<img src=\"https://opensource.muenchen.de/logo/langchain.jpg\" alt=\"Langchain Logo\" style=\"width:300px;\">  \n",
    "\n",
    "Langchain is a powerful framework that abstracts language model providers, allowing seamless integration of various LLMs, including:  \n",
    "\n",
    "- Ollama  \n",
    "- Claude  \n",
    "- OpenAI  \n",
    "\n",
    "In addition to model integration, Langchain offers a range of prebuilt components for common use cases, such as search and agent-based interactions.  \n",
    "\n",
    "To use Ollama with Langchain, simply utilize the `ChatOllama` class from the `langchain_ollama` package.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "model = ChatOllama(model=MODEL ,base_url=\"http://localhost:11434\")\n",
    "\n",
    "messages = [SystemMessage(\"You are a helpfull assistant\"), HumanMessage(\"What are you?\")]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "result.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Your Task: Chatbot with memory\n",
    "\n",
    "Use the [Automatic Message Management](https://python.langchain.com/docs/how_to/chatbots_memory/#automatic-history-management) to let your bot remember the conversation history. This will allow the bot to remember previous messages and respond accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Give me a hello world example in python\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What did I just ask you?\")]},\n",
    "    config={\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")\n",
    "result['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Continue\n",
    "\n",
    "You can now use our [SmolLMV2](https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct) instance as the backend for the Continue plugin.\n",
    "\n",
    "To set it up, follow these steps:\n",
    "\n",
    "1. Open the Continue settings:  \n",
    "   ![](./media/continue/continue_settings.png) ![](./media/continue/continue_file.png)\n",
    "   \n",
    "2. Copy the contents of [`example_config.json`](./example_config.json).  \n",
    "\n",
    "3. Paste them into the \"Configuration\" file in Continue.\n",
    "\n",
    "Once configured, you should be able to use the chat functionality of the Continue plugin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Tab Auto Completions  \n",
    "\n",
    "To enable tab auto-completions (Ghost Tab) in your editor, you'll need to download a tab auto-completion model.  \n",
    "\n",
    "For this setup, we'll use the [Qwen2.5-Coder](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF) model.  \n",
    "\n",
    "Once the model is downloaded, you should start seeing code completion suggestions directly in your editor.  \n",
    "\n",
    "<video width=\"480\" height=\"320\" controls>  \n",
    "  <source src=\"./media/continue/example.mp4\" type=\"video/mp4\">  \n",
    "</video>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "#Download the model\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "client.pull(\"qwen2.5-coder:0.5b-instruct-q4_K_M\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# genrate fibonacci numbers up to a max of 500\n",
    "def fibonacci():"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
