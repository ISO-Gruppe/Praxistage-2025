{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search\n",
    "\n",
    "Semantic search retrieves information by understanding the meaning and context of queries using techniques like vector embeddings, enabling more accurate and relevant results beyond simple keyword matching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embeddings\n",
    "\n",
    "### 1.1 What are Embeddings?\n",
    "\n",
    "Embeddings are N-dimensional vector representations derived from text data. They capture the semantic meaning of the text, enabling similarity comparisons using distance metrics. These representations allow for efficient search and retrieval of relevant information based on textual relationships.\n",
    " \n",
    "\n",
    "![](../obsidian/Excalidraw/Embeddings.excalidraw.svg)\n",
    "\n",
    "\n",
    "### 1.2 Creating Embeddings with Ollama  \n",
    "\n",
    "Just like chat models, embedding models can be easily downloaded and hosted on our own hardware.  \n",
    "\n",
    "The following code snippet demonstrates how to generate embeddings using an Ollama embedding model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client\n",
    "\n",
    "# Download the embedding model\n",
    "MODEL = \"all-minilm:33m\"\n",
    "client = Client(host=\"http://localhost:11434\")\n",
    "client.pull(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for a given text\n",
    "result = client.embed(\n",
    "    model=MODEL,\n",
    "    input=\"Hello world\",\n",
    ")\n",
    "\n",
    "print(f\"{result.embeddings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Calculating Similarities\n",
    "\n",
    "To determine how similar two sentences are, we compare the distance between their embeddings. A common approach is to use **cosine similarity**, which measures the cosine of the angle between two vectors:\n",
    "\n",
    "\n",
    "$\\text{Cosine Similarity} = \\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|\\mathbf{a}| |\\mathbf{b}|}$\n",
    "\n",
    "\n",
    "(An example implementation in Python is provided below.)\n",
    "\n",
    "Visualizing the **similarity matrix** can offer deeper insights into the relationships between sentences. This allows us to identify the most semantically relevant sentences based on their contextual meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Example function to calculate cosine similarity between multiple embeddings\n",
    "def cosine_similarity(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the cosine similarity between all pairs of embedding vectors in a 2D array.\n",
    "\n",
    "    Parameters:\n",
    "        embeddings (numpy.ndarray): 2D array where each row is an embedding.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: 2D array of cosine similarities. The element at [i, j] is the cosine similarity between the ith and jth embeddings.\n",
    "    \"\"\"\n",
    "    # Calculate the Gram matrix (dot product of each pair of embeddings)\n",
    "    gram = np.dot(embeddings, embeddings.T)\n",
    "\n",
    "    # Calculate the norms of each embedding\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "    # Avoid division by zero by ensuring norms are at least a small epsilon\n",
    "    epsilon = np.finfo(float).eps\n",
    "    norms += epsilon\n",
    "\n",
    "    # Compute the cosine similarity\n",
    "    similarity = gram / (norms * norms.T)\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Visualizing Sentence Similarities  \n",
    "**a) Heatmaps**  \n",
    "\n",
    "One effective way to analyze sentence similarities is through **heatmaps**, which provide a clear visual representation of the similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import bulk_embed, plot_similarity_heatmap\n",
    "\n",
    "\n",
    "example_sentences = [\n",
    "    \"The cat sat on the windowsill, watching the birds outside.\",\n",
    "    \"A feline perched by the window, observing the chirping sparrows.\",\n",
    "    \"The dog barked loudly when the mailman arrived.\",\n",
    "    \"A postal worker delivered letters while a nearby canine growled.\",\n",
    "    \"The sun sets in the evening, painting the sky orange and red.\",\n",
    "    \"At dusk, the horizon glows with vibrant shades of crimson and gold.\",\n",
    "    \"She enjoys reading mystery novels late at night.\",\n",
    "    \"At night, she immerses herself in thrilling detective stories.\",\n",
    "    \"The train arrived at the station five minutes late.\",\n",
    "    \"Passengers waited patiently as the delayed locomotive approached.\",\n",
    "]\n",
    "\n",
    "embeddings = bulk_embed(MODEL, example_sentences, client)\n",
    "\n",
    "\n",
    "similarities = cosine_similarity(embeddings)\n",
    "\n",
    "plot_similarity_heatmap(similarities, texts=example_sentences, limit=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) 3D-Plots**\n",
    "\n",
    "Another powerful method for visualizing sentence similarities is **3D plots**, which offer a more dynamic perspective on the relationships between sentence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_embeddings\n",
    "\n",
    "plot_embeddings(embeddings=embeddings, texts=example_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Your Task: Multilingual Similarity Analysis  \n",
    "\n",
    "Your task is to **visualize the similarities between sentences with similar semantic content but in different languages**. This will help assess whether our model effectively captures multilingual semantics.  \n",
    "\n",
    "Once you have completed the analysis, repeat the process using the `granite-embedding:278m` model to compare its performance in handling multilingual embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Vector Stores and Search  \n",
    "\n",
    "A **vector store** is a specialized database for managing and searching high-dimensional embeddings. Unlike traditional keyword-based search, vector stores enable **semantic search**, retrieving relevant information based on meaning rather than exact matches. This is achieved through **approximate nearest neighbor (ANN) search**, which efficiently finds semantically similar entries.  \n",
    "\n",
    "To integrate vector stores into our system, we will again use **LangChain**, which provides seamless tools for storing and retrieving embeddings for effective semantic search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def create_vector_store(model: str = \"granite-embedding:278m\"):\n",
    "    # Wrap our Ollama model\n",
    "    embedding_provider = OllamaEmbeddings(model=model)\n",
    "\n",
    "    # Initialize the vector store\n",
    "    index = faiss.IndexFlatL2(len(embedding_provider.embed_query(\"hello world\")))\n",
    "\n",
    "    # Store the documents in memory for now\n",
    "    vector_store = FAISS(\n",
    "        embedding_function=embedding_provider,\n",
    "        index=index,\n",
    "        docstore=InMemoryDocstore(),\n",
    "        index_to_docstore_id={},\n",
    "    )\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store = create_vector_store()\n",
    "vector_store.distance_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Using the Vector Store  \n",
    "\n",
    "To store text in the vector database, we wrap it in a `Document` object, which includes `page_content` for the text itself and optional `metadata` for additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for i, sentence in enumerate(example_sentences):\n",
    "    document = Document(page_content=sentence, metadata={\"document_id\": i})\n",
    "    documents.append(document)\n",
    "\n",
    "ids = vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Querying the Vector Store  \n",
    "\n",
    "To retrieve similar documents, we can use the `similarity_search_with_score` function, which finds and ranks documents based on their relevance to a given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(query=\"cat\", k=3)\n",
    "for doc, score in results:\n",
    "    print(f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Indexing a Codebase  \n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" alt=\"Langchain Pipeline\" style=\"width:800px;\">  \n",
    "\n",
    "Now, let's work with a real codebase and index it. As an example, we'll use the [TEI-Client](https://github.com/LLukas22/tei-client) repository, which is small and easy to understand.  \n",
    "\n",
    "The code below clones the repository into the [`repo`](./repo/) folder. If you don’t have `git` installed, you can manually download the code from [GitHub](https://github.com/LLukas22/tei-client) and place it in the [`repo`](./repo/) directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from pathlib import Path\n",
    "\n",
    "repo_path = Path(\"repo\").resolve()\n",
    "if not repo_path.exists():\n",
    "    repo_path.mkdir(parents=True)\n",
    "\n",
    "repo_url = \"https://github.com/LLukas22/tei-client.git\"\n",
    "\n",
    "try:\n",
    "    Repo.clone_from(repo_url, \"./repo\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to clone repository: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Loading Code Files  \n",
    "\n",
    "Langchain provides utility functions to efficiently locate and load files as `Document` objects.  \n",
    "\n",
    "The following code demonstrates how to accomplish this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "\n",
    "loader = DirectoryLoader(repo_path, glob=\"**/*.py\", loader_cls=TextLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "for doc in docs:\n",
    "    print(\"_\" * 8)\n",
    "    print(f\"Source: {doc.metadata[\"source\"]}\")\n",
    "    print(f\"Characters: {len(doc.page_content)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Challenges in Embedding Code  \n",
    "\n",
    "When adding code files to the vector store, we may encounter errors due to their large size. This happens because our embedding model has a token limit and cannot process documents exceeding that limit in a single pass.  \n",
    "\n",
    "To resolve this, we need to split our `Document` objects into smaller, manageable chunks before embedding them. This ensures that each chunk stays within the model's token constraints while preserving the overall structure and meaning of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import ResponseError\n",
    "\n",
    "# This will fail if the document exceeds the maximum context length\n",
    "vector_store = create_vector_store()\n",
    "try:\n",
    "    ids = vector_store.add_documents(docs)\n",
    "except ResponseError as e:\n",
    "    if e.status_code == 500:\n",
    "        print(\"Document exceeded the maximum context length\")\n",
    "    else:\n",
    "        print(f\"Failed to add document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Your Task: Properly Splitting and Indexing Code Files  \n",
    "\n",
    "To effectively store and search code files, we need to:  \n",
    "\n",
    "1. **Load the code files** using Langchain’s [`GenericLoader` and `LanguageParser`](https://python.langchain.com/docs/integrations/document_loaders/source_code/). These tools help extract structured information from source code files, making them easier to process.  \n",
    "\n",
    "2. **Split the files into manageable chunks** using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/integrations/document_loaders/source_code/#splitting). This ensures that each piece remains within the embedding model's token limit while maintaining logical code segments for meaningful retrieval.  \n",
    "\n",
    "By following this approach, we can efficiently index the codebase while preserving its readability and searchability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to add the documents to our vector store without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_vector_store()\n",
    "try:\n",
    "    ids = vector_store.add_documents(docs)\n",
    "    print(f\"Added {len(ids)} documents!\")\n",
    "except ResponseError as e:\n",
    "    if e.status_code == 500:\n",
    "        print(\"Document exceeded the maximum context length\")\n",
    "    else:\n",
    "        print(f\"Failed to add document: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Searching the Codebase  \n",
    "\n",
    "With our codebase successfully indexed, we can now perform searches using the vector store. Let's test it with a few queries:  \n",
    "\n",
    "- **\"Where is the `embed` function implemented?\"**  \n",
    "- **\"How is reranking handled?\"**  \n",
    "- **\"How can I create a client?\"**  \n",
    "\n",
    "These queries should return the most relevant code snippets, making it easy to locate specific implementations within the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    query=\"Where is the `embed` function implemented?\", k=5\n",
    ")\n",
    "for doc, score in results:\n",
    "    print(\"-\"*10)\n",
    "    print(f\"* [SIM={score:3f}]\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
