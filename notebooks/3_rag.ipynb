{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)  \n",
    "\n",
    "## 1. What is RAG?  \n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" alt=\"Langchain Pipeline\" style=\"width:800px;\">  \n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances language model responses by retrieving relevant information from an external knowledge source, such as a database or document collection. This allows the model to generate more accurate, contextually relevant, and up-to-date answers while reducing hallucinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Your Task: Build a RAG Pipeline  \n",
    "\n",
    "Your goal is to construct a **Retrieval-Augmented Generation (RAG) pipeline** using a provided `LLM` and `vector_store`.  \n",
    "\n",
    "### Steps to follow:  \n",
    "1. **Retrieve relevant documents** – Fetch the top `n` most relevant documents from the `vector_store` based on a user query.  \n",
    "2. **Generate a response** – Use the `LLM` to process the retrieved documents and generate a well-informed answer.  \n",
    "\n",
    "Refer to Langchain’s [RAG Documentation](https://python.langchain.com/docs/tutorials/rag/#preview) for guidance on implementing this pipeline effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined imports\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "#Load our models\n",
    "llm = ChatOllama(model=\"smollm2:360m\" ,base_url=OLLAMA_URL)\n",
    "embedding_provider = OllamaEmbeddings(model=\"granite-embedding:278m\", base_url=OLLAMA_URL)\n",
    "\n",
    "#Load Vector store from disk\n",
    "vector_store = FAISS.load_local(\"tei-client-index\", embedding_provider, allow_dangerous_deserialization=True)\n",
    "\n",
    "#Define a Prompt to use\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=(\n",
    "        \"You are an assistant for code understanding tasks. \"\n",
    "        \"Use the following pieces of retrieved code to answer the question. \"\n",
    "        \"If you don't know the answer, just say that you don't know. \"\n",
    "        \"Try to answer in markdown syntax.\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your implementation bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the RAG pipeline a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = graph.invoke({\"question\": \"How do i use the embed methode?\"})\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Optimize the Implementation  \n",
    "\n",
    "Enhance the pipeline by adjusting the number of retrieved documents (`n`) or refining the prompt to improve the accuracy of generated answers.  \n",
    "You can also experiment with larger models, such as [`llama3.2`](https://ollama.com/library/llama3.2), to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
