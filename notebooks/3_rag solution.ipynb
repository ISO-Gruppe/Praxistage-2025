{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)  \n",
    "\n",
    "## 1. What is RAG?  \n",
    "\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png\" alt=\"Langchain Pipeline\" style=\"width:800px;\">  \n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that enhances language model responses by retrieving relevant information from an external knowledge source, such as a database or document collection. This allows the model to generate more accurate, contextually relevant, and up-to-date answers while reducing hallucinations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Your Task: Build a RAG Pipeline  \n",
    "\n",
    "Your goal is to construct a **Retrieval-Augmented Generation (RAG) pipeline** using a provided `LLM` and `vector_store`.  \n",
    "\n",
    "### Steps to follow:  \n",
    "1. **Retrieve relevant documents** – Fetch the top `n` most relevant documents from the `vector_store` based on a user query.  \n",
    "2. **Generate a response** – Use the `LLM` to process the retrieved documents and generate a well-informed answer.  \n",
    "\n",
    "Refer to Langchain’s [RAG Documentation](https://python.langchain.com/docs/tutorials/rag/#preview) for guidance on implementing this pipeline effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined imports\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\"\n",
    "\n",
    "#Load our models\n",
    "llm = ChatOllama(model=\"smollm2:360m\" ,base_url=OLLAMA_URL)\n",
    "embedding_provider = OllamaEmbeddings(model=\"granite-embedding:278m\", base_url=OLLAMA_URL)\n",
    "\n",
    "#Load Vector store from disk\n",
    "vector_store = FAISS.load_local(\"tei-client-index\", embedding_provider, allow_dangerous_deserialization=True)\n",
    "\n",
    "#Define a Prompt to use\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=(\n",
    "        \"You are an assistant for code understanding tasks. \"\n",
    "        \"Use the following pieces of retrieved code to answer the question. \"\n",
    "        \"If you don't know the answer, just say that you don't know. \"\n",
    "        \"Try to answer in markdown syntax.\"\n",
    "        \"Question: {question}\\n\"\n",
    "        \"Context: {context}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your implementation bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# Your code goes here\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=10)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(f\"File:{doc.metadata[\"source\"]}\\nContent:{doc.page_content}\" for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the RAG pipeline a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To use the Embed method in a Grpc client, you need to create an EmbedServicer class and implement the embedded methods. The Embed method is used for generating embeddings of text, while the EmbedAll and EmbedStream methods are used for generating embeddings across multiple sources. To make sure that these embedded methods work as expected, you can use the RPCMethodHandlers module from the base code:\n",
       "\n",
       "- Add an EmbedServicer class to the src/tei_client/stubs folder:\n",
       "\n",
       "```python\n",
       "from grpc import StreamServer\n",
       "from gRPC.exceptions import GRPCUnavailableError\n",
       "\n",
       "def _add_EmbedServicer_to_server(servicer, server):\n",
       "    if not hasattr(servicer, \"embed\"):\n",
       "        raise GRPCUnavailableError(\"Embed servcer not registered\")\n",
       "    for method_name in (\"embed\", \"EmbedStream\",):\n",
       "        rpc_method_handlers = {\n",
       "            \"Embed\": servicer.embed,\n",
       "            \"EmbedStream\": servicer.EmbedStream,\n",
       "        }\n",
       "        server.add_rpc_method_handler(method_name, rpc_method_handlers)\n",
       "\n",
       "def embed(\n",
       "    request,\n",
       "    target,\n",
       "    options=(),\n",
       "    channel_credentials=None,\n",
       "    call_credentials=None,\n",
       "    insecure=False,\n",
       "    compression=None,\n",
       "    wait_for_ready=None,\n",
       "    timeout=None,\n",
       "    metadata=None,\n",
       "):\n",
       "    return grpc.experimental.unary_unary(\n",
       "        request,\n",
       "        target,\n",
       "        \"/tei.v1.Embed/Embed\",\n",
       "        tei__pb2.EmbedRequest.SerializeToString,\n",
       "        tei__pb2.EmbedResponse.FromString,\n",
       "        options,\n",
       "        channel_credentials,\n",
       "        insecure,\n",
       "        call_credentials,\n",
       "        compression,\n",
       "        wait_for_ready,\n",
       "        timeout,\n",
       "        metadata,\n",
       "    )\n",
       "```\n",
       "\n",
       "- Add an Embed method to the server class:\n",
       "\n",
       "```python\n",
       "from gRPC import StreamServer\n",
       "from grpc import StreamServerError\n",
       "\n",
       "def _add_Embed(server):\n",
       "    if not hasattr(server, \"embed\"):\n",
       "        raise GRPCUnavailableError(\"Embed servcer not registered\")\n",
       "    for method_name in (\"Embed\",):\n",
       "        server.add_rpc_method_handler(method_name, self._add_EmbedServicer_to_server)\n",
       "\n",
       "def embed(\n",
       "    request,\n",
       "    target,\n",
       "    options=(),\n",
       "    channel_credentials=None,\n",
       "    call_credentials=None,\n",
       "    insecure=False,\n",
       "    compression=None,\n",
       "    wait_for_ready=None,\n",
       "    timeout=None,\n",
       "    metadata=None,\n",
       "):\n",
       "    return grpc.experimental.unary_unary(\n",
       "        request,\n",
       "        target,\n",
       "        \"/tei.v1.Embed/Embed\",\n",
       "        tei__pb2.EmbedRequest.SerializeToString,\n",
       "        tei__pb2.EmbedResponse.FromString,\n",
       "        options,\n",
       "        channel_credentials,\n",
       "        insecure,\n",
       "        call_credentials,\n",
       "        compression,\n",
       "        wait_for_ready,\n",
       "        timeout,\n",
       "        metadata,\n",
       "    )\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "response = graph.invoke({\"question\": \"How do i use the embed methode?\"})\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Optimize the Implementation  \n",
    "\n",
    "Enhance the pipeline by adjusting the number of retrieved documents (`n`) or refining the prompt to improve the accuracy of generated answers.  \n",
    "You can also experiment with larger models, such as [`llama3.2`](https://ollama.com/library/llama3.2), to achieve better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
