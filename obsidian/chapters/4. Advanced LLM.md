# Advanced LLMs

---

## Current state
![[LLM_Hallucinations.excalidraw]]



note: (LLM with no external Knowledge)
LLM are word generators - Systems that create new text from scratch using language models.
Analogy: "Imagine a knowledgeable friend answering based on what they recall."
Example: AI writing a story or answering a prompt creatively.
Strengths:
Produces fluent, natural-sounding text.
Great for open-ended or creative responses.
Weaknesses:
Can make up ("hallucinate") incorrect details.
Limited to knowledge from its training, which might be outdated.

---

## Including knowledge into LLMs
<br>
<split even gap="2">

**1. Finetuning**

**2. Adapters**

**3. "In context" Learning**
</split>

note: 
- explain that an LLM without knowledge about the code base is useless
(Maybe image)
* two AI-based approaches : 
    retrieval-based: semantic search / vector search
generation-based methods—and explain that RAG combines them for better results.

--

### 1. Finetuning
![[Finetuning.excalidraw.svg]]
--

### 2. Adapters
![[Adapters.excalidraw.svg]]
--

### 3. "In Context" Learning

![[In-Context-Learning.excalidraw.svg]]
---
## Semantic Search
![[Semantic_basics.excalidraw.svg]]
> How can we get the matching documents from the DB?

note:
Let's take another example: a RAG for travel recommandations. 

Whan happens when the user is  "I would like to make a citybreak over the weekend. What do you recommend?"

There are two different mechanisms that come into play here:

Semantic Search / vectorisation
Realizes that you are interested to travel to a city, proably interested in culture, night life, and all the other things associated with a citybreak. It picks up terms like “museums”, “galleries”, “concerts", and “recreational activities”

Vector Search
based on the semantic features, can look for locations and finds that in the semantic context of a city break, Berlin, Prag, Paris, Milano, etc have features that match “museums”, “galleries”, “concerts", and “recreational activities”. 

Analogy: "Think of a librarian finding books that match your question."
Example: Search engines like Google, which retrieve web pages based on keywords.

#### Strengths: 

- Access to huge amounts of information

- Can stay current if the database is updated.

#### Weaknesses:
- May return entire documents instead of direct answers.
- Requires users to sift through results for the key information.

Semantic search knows what you mean, and vector search finds things that are alike. Together they get you accurate results of what you had in mind.

Embeddings are vectors in a high-dimensional space. 

Think of text embedding like placing words on a giant map. Words with similar meanings (like "dog" and "puppy") are close together, while unrelated words (like "dog" and "car") are far apart.

--

### Embeddings
![[Embeddings.excalidraw.svg]]
--

 ### <i class="fas fa-book fa-sm"></i> Semantic Search Notebook 

`notebooks\2_semantic_search.ipynb`


---
## Retrieval-Augmented Generation (RAG)

![[https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png]]
note:
"You’ve used code assistants like Copilot, but how do they go from generic suggestions to actually understanding your codebase—like knowing your update_score function isn’t just a counter?"

Out of the box, a Large Language Model (LLM) powering a code assistant relies on its vast training data—public repos, docs, etc.
But for it to "understand" your project—your naming conventions, architecture, and logic—it needs a way to tap into your specific codebase. That’s where RAG comes in.

RAG (Retrieval-Augmented Generation) pairs retrieval of relevant data with text generation. Here, the “data” is your codebase.

An LLM alone has no clue about your private repo’s UserManager class or your custom db.query wrapper—it’s blind to anything not in its training set.
RAG bridges that gap by indexing your code and feeding it into the LLM on demand.

Lookup in Action: RAG’s Workflow
Scenario: You type update_score( in your IDE.

Process:
Query Embedding:
The assistant embeds your input (update_score) into a vector using the same model that indexed your code.

Retrieval:
RAG searches the vector index for the closest matches using cosine similarity or another distance metric.
It finds your update_score function’s embedding (and maybe related snippets, like a caller).
Retrieved:
```
def update_score(user_id, points):
    db.execute("UPDATE users SET score = score + ?", (points,))
```    

Context Injection:
The retrieved code is fed into the LLM as context, alongside your prompt.
Think of it as dynamically expanding the LLM’s memory with your codebase.
Generation:
The LLM generates a suggestion grounded in that context:

```
update_score(42, 10)  # Adds 10 points to user 42
```

Or it might extend it:
```
def reset_score(user_id):
    update_score(user_id, 0)
```

#### Why This is better than a Plain LLM
Precision:

Without RAG, the LLM might suggest a generic update_score that increments a variable—useless if your version hits a database.
RAG ensures suggestions align with your actual implementation.

Relevance:

It respects your project’s conventions—like using db.execute instead of sqlite3.connect.

Scalability:

The index can handle huge codebases, retrieving only the most relevant bits in milliseconds.

Technical details: 

Code-specific models (e.g., CodeBERT) outperform generic text embedders because they understand syntax and semantics—like for loops vs. if blocks.

Chunking:
Your code isn’t indexed as one giant blob. It’s split into logical units (functions, classes) to keep retrieval granular.

Freshness:
The index updates as your codebase changes—edit update_score, and RAG reflects it next time.

## RAG: Retriever + Augmented generation

What It Is: RAG merges retrieval and generation to get the best of both worlds. It retrieves relevant documents and then generates a response based on them.
How It Works:
Query: A user asks a question.
Retrieval: The system searches a database for relevant info.
Generation: The system crafts a response using the query and retrieved data.
Response: A clear, grounded answer is delivered.
Visual Aid Idea: Use a flowchart:
Query → Retrieve Documents → Generate Response → Final Answer

Advantages of RAG
Accuracy: Reduces errors by grounding responses in retrieved documents.
Freshness: Can use up-to-date data from a current database.
Transparency: Can show sources, so students can check the facts.
Flexibility: Works for simple facts or detailed explanations.

Challenges and Limitations
Retrieval Dependence: If it retrieves irrelevant documents, the answer suffers.
Blending Issues: The generated text might not fully match the retrieved info, causing confusion.
Resource Needs: Requires more computing power than simpler methods.

--


 ### <i class="fas fa-book fa-sm"></i> RAG Notebook 

`notebooks\2_rag.ipynb`