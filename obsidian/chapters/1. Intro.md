
# Agenda

1. Intro, who's who and Tech Stack
2. How do LLMs understand and generate text?  
   ðŸ–®   | *environment setup*  
	ðŸ–®   | *running your own chatbot*
4. How to use ISO Coding Assistant?
5. How to include knowledge into LLMs?  
	ðŸ–®  |  *hands-on semantic search*  
	ðŸ–®  |  *building a RAG pipeline*

6. Conclusion

---
## Our Stack


<split even gap="2">
![[vscode.png|200x200]]
![[uv.png|200x200]]
![[python.png|200x200]]
</split>
<split even gap="2">
![[ollama.png|200x200]]
![[langchain.png|200x200]]
</split>

--
### IDE: Visual Studio Code

![[vscode.png|300x300]]

> Lightweight, Customizable, Extensible, Powerful, Cross-platform

[https://code.visualstudio.com/download](https://code.visualstudio.com/download)

--
### Python & `UV` Package Management

<split even gap="2">
![[python.png|200x200]]
![[uv.png|200x200]]
</split>

> Fast, Cross-platform python & pip package management

[https://docs.astral.sh/uv/getting-started/installation/](https://docs.astral.sh/uv/getting-started/installation/)

--
### Ollama
![[ollama.png|300x300]]
> Easily download & run modern LLMs locally

[https://ollama.com/download](https://ollama.com/download)
--
### LangChain
![[langchain.png|300x300]]
> Framework for LLM app development

[https://python.langchain.com](https://python.langchain.com)


